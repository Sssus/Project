{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-0df5408cb4a3>, line 146)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-0df5408cb4a3>\"\u001b[1;36m, line \u001b[1;32m146\u001b[0m\n\u001b[1;33m    While True:\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Nov  9 17:29:50 2019\n",
    "\n",
    "@author: Sungmin Hong\n",
    "\"\"\"\n",
    "#%%\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from pymongo import MongoClient\n",
    "\n",
    "#%%\n",
    "# Global Variable\n",
    "docNum = 0\n",
    "#%%\n",
    "# REPLACE With your LinkedIn Credentials\n",
    "USERNAME = \"\"\n",
    "PASSWORD = \"\"\n",
    "#%%\n",
    "def mongodb_init():\n",
    "    client=MongoClient('mongodb://localhost')\n",
    "    db=client.smartcareer\n",
    "    return db\n",
    "#%%\n",
    "def mongodb_get_collection(db,item):\n",
    "    col=db[item]\n",
    "    return col\n",
    "#%%\n",
    "def mongodb_put_doc(doc):\n",
    "    db=mongodb_init()\n",
    "    col=mongodb_get_collection(db,'dataengineer')\n",
    "\n",
    "    try:\n",
    "        global docNum\n",
    "        re=col.insert_one(doc)\n",
    "        ret=re.inserted_id    ##re에 id 번호 부여하는 것?\n",
    "        docNum += 1\n",
    "    except:\n",
    "        ret=doc['ProfileID']\n",
    "          \n",
    "    return ret\n",
    "#%%\n",
    "def clean_item(item):\n",
    "    item = item.replace('\\n', ' ')\n",
    "    item = item.strip()\n",
    "    return item\n",
    "#%%\n",
    "# https://www.linkedin.com/search/results/people/?facetGeoRegion=%5B%22us%3A0%22%5D&keywords=Data%20Engineer&origin=FACETED_SEARCH\n",
    "#검색하고자 하는 정보를 title를 넣어 기본 검색 URL을 만든다.\n",
    "# 잡타이틀 정보에 불필요한 공백, 특수문자를 제거한다.\n",
    "# 내가 가져올 url을 넘겨준다. scrape_url\n",
    "def generate_scrape_url(scrape_url, config):\n",
    "\n",
    "    title = config['Job Title']  ## config는 cfg.json전체 정보.\n",
    "\n",
    "# collecting people in US only\n",
    "    scrape_url += \"/search/results/people/?facetGeoRegion=%5B%22us%3A0%22%5D&keywords=\"\n",
    "    scrape_url += title\n",
    "    scrape_url += \"&origin=SWITCH_SEARCH_VERTICAL\"\n",
    "\n",
    "\n",
    "    valid_title_name = title.strip().replace(' ', '_')\n",
    "    valid_title_name = re.sub(r'(?u)[^-\\w.]', '', valid_title_name) #means 첫번째를 2번째로 바꾼다. valid_title_name에서\n",
    "\n",
    "    return scrape_url\n",
    "#%%\n",
    "def scroll_down_page(browser, speed=8): \n",
    "    current_scroll_position, new_height = 0, 1\n",
    "    while current_scroll_position <= new_height:\n",
    "        current_scroll_position += speed\n",
    "        browser.execute_script(\"window.scrollTo(0, {});\".format(current_scroll_position))\n",
    "        new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "#%%\n",
    "def pscrape(config):\n",
    "    start_time = time.time()\n",
    "    # driver download: https://github.com/mozilla/geckodriver/releases\n",
    "    # windows \\, Linux and Max /\n",
    "    driver = os.getcwd() + \"/geckodriver.exe\"\n",
    "    base_url = \"https://www.linkedin.com\"\n",
    "    sign_in_url = \"https://www.linkedin.com/uas/login?fromSignIn=true\"\n",
    "    people_data = []\n",
    "    page = 40\n",
    "\n",
    "    USERNAME = config['User Name']\n",
    "    PASSWORD = config['Password']\n",
    "    col=config['Collection']\n",
    "    \n",
    "    # 주어진 json파일정보로 검색 url을 얻기\n",
    "    people_search_url = generate_scrape_url(base_url, config)\n",
    "\n",
    "    # 해당 URL 로 이동 (로그인 페이지 이동)\n",
    "    print('\\nSTATUS: Opening website')\n",
    "    browser = webdriver.Firefox(executable_path=driver)  ##내가 Firefox에서 Chrome으로 바꿈\n",
    "    browser.get(sign_in_url)\n",
    "    time.sleep(random.randint(1,3))\n",
    "\n",
    "    # 해당 페이지에서 유저명과 패스워드를 입력.\n",
    "    print('STATUS: Signing in')\n",
    "    browser.find_element_by_id('username').send_keys(USERNAME)\n",
    "    time.sleep(random.randint(1,3))\n",
    "\n",
    "    browser.find_element_by_id('password').send_keys(PASSWORD)\n",
    "    time.sleep(random.randint(1,3))\n",
    "\n",
    "    # 로그인 버튼 클릭\n",
    "    browser.find_element_by_class_name('login__form_action_container ').click()\n",
    "    time.sleep(random.randint(1,3))\n",
    " \n",
    "    # 로그인 이후에 json에 주어진 jobtitle로 검색을 수행한다. 수행 후, 스크롤해주기\n",
    "    print('STATUS: Searching for people\\n')\n",
    "    browser.get(people_search_url)\n",
    "    time.sleep(random.randint(1,3))\n",
    "    scroll_down_page(browser, 4)\n",
    "\n",
    "    # 사람 정보 위치(아마도)\n",
    "    people = browser.find_elements_by_xpath('//div[@class=\"search-result__info pt3 pb4 ph0\"]')\n",
    "\n",
    "    \n",
    "    if len(people) == 0:  # 만약 원하는 위치에 정보가 없으면\n",
    "        print('STATUS: No people found. Press any key to exit scraper')\n",
    "        print(\"Check docnum.txt for # of documents submitted!\")\n",
    "        # docnum.txt를 만들어서(또는 열어서) 정보를 써준다.\n",
    "        today = datetime.now()\n",
    "        f = open(\"docnum1.txt\",\"w+\")  ##docnum.txt 없으면 생기고, 쓰기 기능으로(+ 조사 필요)들어놓아야 하는지? 코드와 같은 Anaconda Folder 안에-\n",
    "        f.write(\"Ran:\\n\")\n",
    "        f.write(str(today))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\nNumber of documents submitted:\\n\")\n",
    "        f.write(str(docNum))\n",
    "        f.close()\n",
    "        browser.quit()\n",
    "        exit = input('')  # 어떤 역할인지? 우선은 skip(우선은 사용하는 곳이 없어보임.)\n",
    "        sys.exit(0)  # \n",
    "\n",
    "    # 위치에 정보가 있으면\n",
    "    \n",
    "    #Scraping up to 6 pages per search.\n",
    "    While True:\n",
    "        print('\\nSTATUS: Scraping Page ' + str(page))\n",
    "        \n",
    "        # 링크가져오기\n",
    "        links = []\n",
    "        for link in people:\n",
    "            try:\n",
    "                link = link.find_element_by_class_name(\"search-result__result-link\").get_attribute(\"href\")\n",
    "                links.append(link)\n",
    "            except:\n",
    "                ()\n",
    "                \n",
    "        # 링크를 얻고 바로 그 주소로 이동.(획득한 링크만큼)\n",
    "        for link in links:\n",
    "            obj = {}  # 딕셔너리 형태로 빈 딕셔너리 생성.\n",
    "            browser.get(link)\n",
    "            time.sleep(random.randint(1,3))\n",
    "\n",
    "            # 20초동안 스크롤을 한다.\n",
    "            scroll_down_page(browser, 20)\n",
    "  \n",
    "            # 아마도, 링크안에 ProfileID 관련 정보가 있다. 그래서 그 정보를 가져오기\n",
    "            # obj : ProfileID 키 추가, \n",
    "            # case1. 값이 만약 people일 아닌 경우,\n",
    "            obj['ProfileID'] = link.split('/')[len(link.split('/'))-2]\n",
    "            if obj['ProfileID'] != \"people\":\n",
    "                print(\"STATUS: Scraping Profile_ID: {}\".format(obj['ProfileID']))\n",
    "\n",
    "                # 무언가 클릭을 두번 한다. \n",
    "                try:\n",
    "                    browser.find_element_by_xpath(\"//a[@class='lt-line-clamp__more']\").click()\n",
    "                except:\n",
    "                    ()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath(\"//button[@class='pv-profile-section__see-more-inline pv-profile-section__text-truncate-toggle link link-without-hover-state']\").click()\n",
    "                except:\n",
    "                    ()\n",
    "\n",
    "                # 어떤 요소의 Job Title를 text로 가져온다.\n",
    "                try:\n",
    "                    obj['Job Title'] = clean_item(browser.find_element_by_xpath(\"//h2[@class='mt1 t-18 t-black t-normal']\").text)\n",
    "                except:\n",
    "                    obj['Job Title'] = ''\n",
    "\n",
    "                # 어떤 요소의 Location을 가져온다. \n",
    "                try:\n",
    "                    obj['Location'] = clean_item(browser.find_element_by_xpath(\"//li[@class='t-16 t-black t-normal inline-block']\").text)\n",
    "                except:\n",
    "                    obj['Location'] = ''\n",
    "\n",
    "                # Profile Summary를 가져오기 \n",
    "                try:\n",
    "                    obj['Profile Summary'] = clean_item(browser.find_element_by_class_name(\"pv-about__summary-text\").text)\n",
    "                except:\n",
    "                    obj['Profile Summary'] = ''\n",
    "                \n",
    "                # 회사를 가져오기(근무했던 했던 회사가 아닐까?)\n",
    "                try:\n",
    "                    companies = browser.find_element_by_id(\"experience-section\").find_elements_by_class_name(\"pv-profile-section__card-item-v2\")\n",
    "                except:\n",
    "                    companies = []\n",
    "\n",
    "                experience = []\n",
    "                experience_obj = {}\n",
    "                \n",
    "                # 회사들(경력)에서 상세 정보가져오기\n",
    "                for company in companies:\n",
    "                    try:\n",
    "                        experience_obj['Job Title'] = clean_item(company.find_element_by_tag_name('h3').text)\n",
    "                    except:\n",
    "                        experience_obj['Job Title'] = ''\n",
    "\n",
    "                    try:\n",
    "                        experience_obj['Company'] = clean_item(company.find_element_by_class_name('pv-entity__secondary-title').text)\n",
    "                    except:\n",
    "                        experience_obj['Company'] = ''\n",
    "\n",
    "                    try:\n",
    "                        experience_obj['Period'] = clean_item(company.find_element_by_class_name('pv-entity__date-range').text).replace('Dates Employed ', '')\n",
    "                    except:\n",
    "                        experience_obj['Period'] = ''\n",
    "\n",
    "                    try:\n",
    "                        experience_obj['Years'] = clean_item(company.find_element_by_class_name(\"pv-entity__bullet-item-v2\").text)\n",
    "                    except:\n",
    "                        experience_obj['Years'] = ''\n",
    "\n",
    "                    try:\n",
    "                        experience_obj['Location'] = clean_item(company.find_element_by_class_name('pv-entity__location').text).replace('Location ', '')\n",
    "                    except:\n",
    "                        experience_obj['Location'] = ''\n",
    "\n",
    "                    try:\n",
    "                        experience_obj['Description'] = clean_item(company.find_element_by_class_name('pv-entity__description').text)\n",
    "                    except:\n",
    "                        experience_obj['Description'] = ''\n",
    "\n",
    "                    experience.append(experience_obj)\n",
    "                    experience_obj = {}\n",
    "\n",
    "                obj['Experience'] = experience\n",
    "\n",
    "                # 어떤 교육(대학 또는 기관)을 받았는가?\n",
    "                try:\n",
    "                    institutes = browser.find_element_by_id(\"education-section\").find_elements_by_class_name(\"pv-entity__summary-info\")\n",
    "                except:\n",
    "                    institutes = []\n",
    "\n",
    "                education = []\n",
    "                education_obj = {}\n",
    "\n",
    "                # 어떤 학교?(school) 전공일 것 같음. 아마. \n",
    "                for institute in institutes:\n",
    "                    try:\n",
    "                        education_obj['School'] = clean_item(institute.find_element_by_xpath(\"//h3[@class='pv-entity__school-name t-16 t-black t-bold']\").text)\n",
    "                    except:\n",
    "                        education_obj['School'] = ''\n",
    "\n",
    "                    try:\n",
    "                        degree_name = clean_item(institute.find_element_by_class_name('pv-entity__degree-name').text).replace('Degree Name ', '')\n",
    "                    except:\n",
    "                        degree_name = ''\n",
    "\n",
    "                    try:\n",
    "                        field_of_study = clean_item(institute.find_element_by_class_name('pv-entity__fos').text).replace('Field Of Study ', '')\n",
    "                    except:\n",
    "                        field_of_study = ''\n",
    "\n",
    "                    try:\n",
    "                        grade = clean_item(institute.find_element_by_class_name('pv-entity__grade').text).replace('Grade ', '')\n",
    "                    except:\n",
    "                        grade = ''\n",
    "\n",
    "                    education_obj['Degree'] = degree_name+' '+field_of_study+' '+grade\n",
    "\n",
    "                    try:\n",
    "                        education_obj['Date Attended'] = clean_item(institute.find_element_by_class_name('pv-entity__dates').text).replace('Dates attended or expected graduation ', '')\n",
    "                    except:\n",
    "                        education_obj['Date Attended'] = ''\n",
    "\n",
    "                    education.append(education_obj)\n",
    "                    education_obj = {}\n",
    "\n",
    "                obj['Education'] = education\n",
    "            \n",
    "                try:\n",
    "                    browser.find_element_by_xpath(\"//button[@class='pv-profile-section__card-action-bar pv-skills-section__additional-skills artdeco-container-card-action-bar artdeco-button artdeco-button--tertiary artdeco-button--3 artdeco-button--fluid']\").click()\n",
    "                except:\n",
    "                    print(\"No Skills\")\n",
    "\n",
    "                try:\n",
    "                    skill_sets = browser.find_element_by_class_name(\"pv-skill-categories-section__top-skills\").find_elements_by_class_name(\"pv-skill-category-entity__skill-wrapper\")\n",
    "                except:\n",
    "                    skill_sets = []\n",
    "\n",
    "                skills = []\n",
    "                skills_obj = {}\n",
    "\n",
    "                # 당신이 가진 기술은 (아마 확인 필요. Python, R, Spark, ...)\n",
    "                for skill_set in skill_sets:\n",
    "                    try:\n",
    "                        skills_obj['Skills'] = clean_item(skill_set.find_element_by_class_name('pv-skill-category-entity__name').text)\n",
    "                    except:\n",
    "                        skills_obj['Skills'] = ''\n",
    "\n",
    "                    skills.append(skills_obj)\n",
    "                    skills_obj = {}\n",
    "\n",
    "                obj['Skills & Endorsements'] = skills\n",
    "\n",
    "                try:\n",
    "                    all_skillsets = browser.find_elements_by_xpath(\"//div[@class='pv-skill-category-list pv-profile-section__section-info mb6 ember-view']\")\n",
    "                except:\n",
    "                    all_skillsets = []\n",
    "\n",
    "                # 스킬들에 정보를 하나씩 뽑아내기\n",
    "                for one in all_skillsets:\n",
    "                    general_skills = []\n",
    "                    try:\n",
    "                        g_skills = one.find_elements_by_class_name(\"pv-skill-category-entity\")\n",
    "                        for g_skill in g_skills:\n",
    "                            try:\n",
    "                                skills_obj['Skills'] = clean_item(\n",
    "                                    g_skill.find_element_by_class_name('pv-skill-category-entity__name').text)\n",
    "                            except:\n",
    "                                skills_obj['Skills'] = ''\n",
    "\n",
    "                            general_skills.append(skills_obj)\n",
    "                            skills_obj = {}\n",
    "                    except:\n",
    "                        general_skills = []\n",
    "\n",
    "                    try:\n",
    "                        category_name = clean_item(one.find_element_by_tag_name('h3').text)\n",
    "                    except:\n",
    "                        category_name = \"Skills\"\n",
    "\n",
    "                    obj[category_name] = general_skills\n",
    "\n",
    "                try:\n",
    "                    dateCaptured = date.today()\n",
    "                    obj['Date Captured'] = str(dateCaptured)\n",
    "                except:\n",
    "                    obj['Date Captured'] = ''\n",
    "\n",
    "                # 이제 모아진 정보 (obj에 모였음) 이를 mongodb에 넣는다.\n",
    "                people_data.append(obj)\n",
    "            \n",
    "                doc_id=mongodb_put_doc(obj)   # 넣기\n",
    "                print('post id: ', doc_id)\n",
    "            else:\n",
    "                print(\"STATUS: Skipping Profile_ID: {}\".format(obj['ProfileID']))\n",
    "\n",
    "        page += 1\n",
    "        \n",
    "        # 다음 페이지 넘기기\n",
    "        next_page = people_search_url + '&page=' + str(page)\n",
    "        browser.get(next_page)\n",
    "        time.sleep(random.randint(1,3))\n",
    "        scroll_down_page(browser, 4)\n",
    "\n",
    "\n",
    "        people = browser.find_elements_by_xpath('//div[@class=\"search-result__info pt3 pb4 ph0\"]')\n",
    "        if len(people) == 0:\n",
    "            break\n",
    "\n",
    "        # 앞의 while문에 의해서 아마 6페이지를 가져오는 소스 코드임.\n",
    "    browser.quit()\n",
    "#%%\n",
    "import os\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Reads in the config file so input is automatic.\n",
    "    with open(\"cfg.json\") as json_cfg:\n",
    "        d = json.load(json_cfg)\n",
    "    \n",
    "    jobTitle = d['Job Title']\n",
    "\n",
    "    jobList = []\n",
    "    tempStr = \"\"\n",
    "\n",
    "    pscrape(d)\n",
    "    # while len(configArray) > 0:\n",
    "    #     try:\n",
    "    #         int(configArray[0])\n",
    "    #         break\n",
    "    #     except:\n",
    "    #         jobList.append(configArray[0])\n",
    "    #         del configArray[0]\n",
    "\n",
    "    # # Searches multiple jobs in a row.\n",
    "    # for x in range(len(jobList)):\n",
    "    #     print(\"Now scraping:\", jobList[0])\n",
    "    #     pscrape(jobList[0], configArray)\n",
    "    #     time.sleep(5)\n",
    "    #     del jobList[0]\n",
    "\n",
    "    print(\"Daily automation has been completed for: get_people.py\")\n",
    "    print(\"Check docnum.txt for # of documents submitted!\")\n",
    "    today = datetime.now()\n",
    "    f = open(\"docnum.txt\",\"w+\")\n",
    "    f.write(\"Ran:\\n\")\n",
    "    f.write(str(today))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\nNumber of documents submitted:\\n\")\n",
    "    f.write(str(docNum))\n",
    "    f.close()\n",
    "\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "\n",
      "hi2\n"
     ]
    }
   ],
   "source": [
    "print('hi')\n",
    "print('\\nhi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
