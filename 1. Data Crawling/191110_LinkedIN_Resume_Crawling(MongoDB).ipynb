{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STATUS: Opening website\n",
      "STATUS: Signing in\n",
      "STATUS: Searching for people\n",
      "\n",
      "STATUS: Scraping Page 1\n",
      "STATUS: Scraping Profile_ID: emmabobaekong\n",
      "post id:  5dc67f352d165786280a5e89\n",
      "STATUS: Scraping Profile_ID: jinwoo-kang-097b0977\n",
      "post id:  5dc67f512d165786280a5e8b\n",
      "STATUS: Scraping Profile_ID: suzannesoojungbae\n",
      "post id:  5dc67f692d165786280a5e8d\n",
      "STATUS: Scraping Profile_ID: hayliekim\n",
      "post id:  5dc67f832d165786280a5e8f\n",
      "STATUS: Scraping Profile_ID: angeline-peng\n",
      "post id:  5dc67f9e2d165786280a5e91\n",
      "STATUS: Scraping Profile_ID: ieum-han-365533b4\n",
      "No Skills\n",
      "post id:  5dc67fc12d165786280a5e93\n",
      "STATUS: Scraping Profile_ID: jihye-park-5283a6135\n",
      "post id:  5dc67fda2d165786280a5e95\n",
      "STATUS: Scraping Profile_ID: kellyyeonjikim1027\n",
      "No Skills\n",
      "post id:  5dc67fec2d165786280a5e97\n",
      "STATUS: Scraping Profile_ID: aneceahn\n",
      "post id:  5dc67fff2d165786280a5e99\n",
      "STATUS: Scraping Profile_ID: shelbykim\n",
      "post id:  5dc680182d165786280a5e9b\n",
      "STATUS: Scraping Page 2\n",
      "STATUS: Scraping Profile_ID: reywon\n",
      "post id:  5dc680772d165786280a5e9d\n",
      "STATUS: Scraping Profile_ID: sadie-park-572a84132\n",
      "No Skills\n",
      "post id:  5dc680862d165786280a5e9f\n",
      "STATUS: Scraping Profile_ID: joy-han-601b7910b\n",
      "post id:  5dc680d92d165786280a5ea1\n",
      "STATUS: Scraping Profile_ID: michelle-shen\n",
      "No Skills\n",
      "post id:  5dc6810d2d165786280a5ea3\n",
      "STATUS: Scraping Profile_ID: ziyilu\n",
      "post id:  5dc681282d165786280a5ea5\n",
      "STATUS: Scraping Profile_ID: kuangamy\n",
      "post id:  5dc681472d165786280a5ea7\n",
      "STATUS: Scraping Profile_ID: hayley-kim-322a33108\n",
      "No Skills\n",
      "post id:  5dc6815a2d165786280a5ea9\n",
      "STATUS: Scraping Page 3\n",
      "STATUS: Scraping Profile_ID: heather-huh\n",
      "post id:  5dc6819d2d165786280a5eab\n",
      "STATUS: Scraping Profile_ID: sujin-lee-437a52a4\n",
      "post id:  5dc681dd2d165786280a5ead\n",
      "STATUS: Scraping Profile_ID: jiaxuan-han\n",
      "No Skills\n",
      "post id:  5dc681f92d165786280a5eaf\n",
      "STATUS: Scraping Profile_ID: john-paul-jones-493b8459\n",
      "post id:  5dc682112d165786280a5eb1\n",
      "STATUS: Scraping Profile_ID: minoh0224\n",
      "No Skills\n",
      "post id:  5dc6827b2d165786280a5eb3\n",
      "STATUS: Scraping Profile_ID: yuying-q-689228125\n",
      "No Skills\n",
      "post id:  5dc682a22d165786280a5eb5\n",
      "STATUS: Scraping Profile_ID: dianeseo\n",
      "post id:  5dc682c52d165786280a5eb7\n",
      "STATUS: Scraping Profile_ID: amyyeung11\n",
      "post id:  5dc682e52d165786280a5eb9\n",
      "STATUS: Scraping Profile_ID: mmwei\n",
      "post id:  5dc682fb2d165786280a5ebb\n",
      "STATUS: Scraping Profile_ID: hanqing-carol-jiang-092530119\n",
      "No Skills\n",
      "post id:  5dc683132d165786280a5ebd\n",
      "STATUS: Scraping Page 4\n",
      "STATUS: Scraping Profile_ID: meiying-li-b5708340\n",
      "post id:  5dc6834d2d165786280a5ebf\n",
      "STATUS: Scraping Profile_ID: runqi-vanessa-fan\n",
      "post id:  5dc683672d165786280a5ec1\n",
      "STATUS: Scraping Profile_ID: anna-kang-44213491\n",
      "post id:  5dc683832d165786280a5ec3\n",
      "STATUS: Scraping Profile_ID: heejin-helene-kim-2bab894b\n",
      "post id:  5dc6839b2d165786280a5ec5\n",
      "STATUS: Scraping Profile_ID: yerikim0207\n",
      "post id:  5dc683b32d165786280a5ec7\n",
      "STATUS: Scraping Profile_ID: sangji-lee-58766ba2\n",
      "post id:  5dc683cc2d165786280a5ec9\n",
      "STATUS: Scraping Profile_ID: crystal-ye-708044b4\n",
      "post id:  5dc6841b2d165786280a5ecb\n",
      "STATUS: Scraping Profile_ID: hbko1009\n",
      "post id:  5dc684452d165786280a5ecd\n",
      "STATUS: Scraping Profile_ID: mandy-chan\n",
      "post id:  5dc684662d165786280a5ecf\n",
      "STATUS: Scraping Profile_ID: hyesun-kong-0a4292b2\n",
      "post id:  5dc684ad2d165786280a5ed1\n",
      "STATUS: Scraping Page 5\n",
      "STATUS: Scraping Profile_ID: cateliu\n",
      "post id:  5dc684f12d165786280a5ed3\n",
      "STATUS: Scraping Profile_ID: scarlett-kim-b32202b6\n",
      "post id:  5dc685082d165786280a5ed5\n",
      "STATUS: Scraping Profile_ID: chaewon-moon-49b18580\n",
      "post id:  5dc685212d165786280a5ed7\n",
      "STATUS: Scraping Profile_ID: jeanyoon1103\n",
      "post id:  5dc6853b2d165786280a5ed9\n",
      "STATUS: Scraping Profile_ID: changfu-sun-87b259140\n",
      "post id:  5dc685502d165786280a5edb\n",
      "STATUS: Scraping Profile_ID: kayla-ming-742029103\n",
      "post id:  5dc685672d165786280a5edd\n",
      "STATUS: Scraping Profile_ID: subin-kwon-92a4a6105\n",
      "post id:  5dc685812d165786280a5edf\n",
      "STATUS: Scraping Profile_ID: christie-s-wang\n",
      "post id:  5dc6859a2d165786280a5ee1\n",
      "STATUS: Scraping Profile_ID: yeseul-jessica-park-0979b2117\n",
      "No Skills\n",
      "post id:  5dc685ad2d165786280a5ee3\n",
      "STATUS: Scraping Profile_ID: koolhun17\n",
      "post id:  5dc685ca2d165786280a5ee5\n",
      "STATUS: Scraping Page 6\n",
      "STATUS: Scraping Profile_ID: cathy-gong-cfa\n",
      "post id:  5dc685ff2d165786280a5ee7\n",
      "STATUS: Scraping Profile_ID: ji-l-775960139\n",
      "post id:  5dc6860c2d165786280a5ee9\n",
      "STATUS: Scraping Profile_ID: ele-hagermoser\n",
      "post id:  5dc686252d165786280a5eeb\n",
      "STATUS: Scraping Profile_ID: avaz-dadgar-9140bb90\n",
      "post id:  5dc686372d165786280a5eed\n",
      "STATUS: Scraping Profile_ID: sang-park-9161516b\n",
      "post id:  5dc686542d165786280a5eef\n",
      "STATUS: Scraping Profile_ID: shirleyguo1\n",
      "post id:  5dc6866e2d165786280a5ef1\n",
      "STATUS: Scraping Profile_ID: annanohyoo\n",
      "No Skills\n",
      "post id:  5dc686c92d165786280a5ef3\n",
      "STATUS: Scraping Profile_ID: jiyoungkim0307\n",
      "post id:  5dc686fc2d165786280a5ef5\n",
      "STATUS: Scraping Profile_ID: jisoo-lee-10520249\n",
      "post id:  5dc687142d165786280a5ef7\n",
      "STATUS: Scraping Profile_ID: jiangxinzhe\n",
      "post id:  5dc687652d165786280a5ef9\n",
      "STATUS: Scraping Page 7\n",
      "STATUS: Scraping Profile_ID: jeanne-hou\n",
      "post id:  5dc687a22d165786280a5efb\n",
      "STATUS: Scraping Profile_ID: woochanlee\n",
      "post id:  5dc687c32d165786280a5efd\n",
      "STATUS: Scraping Profile_ID: kathy-kim-96a44184\n",
      "post id:  5dc687dc2d165786280a5eff\n",
      "STATUS: Scraping Profile_ID: mjkarukim\n",
      "post id:  5dc688012d165786280a5f01\n",
      "STATUS: Scraping Profile_ID: ge-wang-084627a1\n",
      "post id:  5dc6881c2d165786280a5f03\n",
      "STATUS: Scraping Profile_ID: wai-h-1a046943\n",
      "post id:  5dc688322d165786280a5f05\n",
      "STATUS: Scraping Profile_ID: lmj4841\n",
      "post id:  5dc688492d165786280a5f07\n",
      "STATUS: Scraping Profile_ID: jangsea-park-627a7933\n",
      "post id:  5dc688642d165786280a5f09\n",
      "STATUS: Scraping Profile_ID: lu-tico-han\n",
      "post id:  5dc688aa2d165786280a5f0b\n",
      "STATUS: Scraping Profile_ID: dan-ah-kim-5785b8ba\n",
      "post id:  5dc688c62d165786280a5f0d\n",
      "STATUS: Scraping Page 8\n",
      "STATUS: Scraping Profile_ID: shelby-allen-469366116\n",
      "post id:  5dc689042d165786280a5f0f\n",
      "STATUS: Scraping Profile_ID: josepmaria-palli-mato-454626183\n",
      "No Skills\n",
      "post id:  5dc689192d165786280a5f11\n",
      "STATUS: Scraping Profile_ID: daniel-chung-b46943177\n",
      "post id:  5dc689342d165786280a5f13\n",
      "STATUS: Scraping Profile_ID: charleskorn\n",
      "post id:  5dc6894c2d165786280a5f15\n",
      "STATUS: Scraping Profile_ID: cherinkim13\n",
      "No Skills\n",
      "post id:  5dc689952d165786280a5f17\n",
      "STATUS: Scraping Profile_ID: hylinn\n",
      "post id:  5dc689d82d165786280a5f19\n",
      "STATUS: Scraping Profile_ID: sarahsrsong\n",
      "post id:  5dc689f52d165786280a5f1b\n",
      "STATUS: Scraping Profile_ID: starryzhou\n",
      "post id:  5dc68a152d165786280a5f1d\n",
      "STATUS: Scraping Profile_ID: jasmine-hsu\n",
      "post id:  5dc68a322d165786280a5f1f\n",
      "STATUS: Scraping Profile_ID: heejin-kim-1568a365\n",
      "No Skills\n",
      "post id:  5dc68a4c2d165786280a5f21\n",
      "STATUS: Scraping Page 9\n",
      "STATUS: Scraping Profile_ID: thomaswjkim\n",
      "post id:  5dc68a982d165786280a5f23\n",
      "STATUS: Scraping Profile_ID: jeremy-hsueh-219731109\n",
      "No Skills\n",
      "post id:  5dc68aac2d165786280a5f25\n",
      "STATUS: Scraping Profile_ID: alivelshi\n",
      "post id:  5dc68acb2d165786280a5f27\n",
      "STATUS: Scraping Profile_ID: christinehakyungkim\n",
      "post id:  5dc68b542d165786280a5f29\n",
      "STATUS: Scraping Profile_ID: emilyyukyungchoi\n",
      "No Skills\n",
      "post id:  5dc68b722d165786280a5f2b\n",
      "STATUS: Scraping Profile_ID: maggie-lin-3b9266135\n",
      "post id:  5dc68b922d165786280a5f2d\n",
      "STATUS: Scraping Profile_ID: yinghan-liu\n",
      "post id:  5dc68bd92d165786280a5f2f\n",
      "STATUS: Scraping Profile_ID: yc-park\n",
      "post id:  5dc68bf92d165786280a5f31\n",
      "STATUS: Scraping Profile_ID: michellechang168\n",
      "post id:  5dc68c192d165786280a5f33\n",
      "STATUS: Scraping Profile_ID: joley-schneider-1289b6138\n",
      "post id:  5dc68c512d165786280a5f35\n",
      "STATUS: Scraping Page 10\n",
      "STATUS: Scraping Profile_ID: jennyhnam\n",
      "post id:  5dc68c942d165786280a5f37\n",
      "STATUS: Scraping Profile_ID: youngahy\n",
      "post id:  5dc68cb92d165786280a5f39\n",
      "STATUS: Scraping Profile_ID: lma1230\n",
      "post id:  5dc68cd62d165786280a5f3b\n",
      "STATUS: Scraping Profile_ID: dana-lee-b97903126\n",
      "post id:  5dc68d012d165786280a5f3d\n",
      "STATUS: Scraping Profile_ID: minji09\n",
      "post id:  5dc68d232d165786280a5f3f\n",
      "STATUS: Scraping Profile_ID: tanayrabanks\n",
      "post id:  5dc68d632d165786280a5f41\n",
      "STATUS: Scraping Profile_ID: seohyun-susie-joh-b974b647\n",
      "post id:  5dc68d7a2d165786280a5f43\n",
      "STATUS: Scraping Profile_ID: regina-mitter-257b7040\n",
      "post id:  5dc68d942d165786280a5f45\n",
      "STATUS: Scraping Profile_ID: trangng29\n",
      "post id:  5dc68db62d165786280a5f47\n",
      "STATUS: Scraping Profile_ID: jae-jun-kim-769b5ba7\n",
      "No Skills\n",
      "post id:  5dc68dcf2d165786280a5f49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily automation has been completed for: get_people.py\n",
      "Check docnum.txt for # of documents submitted!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3333: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Nov  9 17:29:50 2019\n",
    "\n",
    "@author: Sungmin Hong\n",
    "\"\"\"\n",
    "#%%\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from pymongo import MongoClient\n",
    "#%%\n",
    "# Global Variable\n",
    "docNum = 0\n",
    "#%%\n",
    "# REPLACE With your LinkedIn Credentials\n",
    "USERNAME = \"\"\n",
    "PASSWORD = \"\"\n",
    "#%%\n",
    "def mongodb_init():\n",
    "    client=MongoClient('mongodb://localhost')\n",
    "    db=client.smartcareer\n",
    "    return db\n",
    "#%%\n",
    "def mongodb_get_collection(db,item):\n",
    "    col=db[item]\n",
    "    return col\n",
    "#%%\n",
    "def mongodb_put_doc(doc):\n",
    "    db=mongodb_init()\n",
    "    col=mongodb_get_collection(db,'applicantprofile_analyst')\n",
    "\n",
    "    try:\n",
    "        global docNum\n",
    "        re=col.insert_one(doc)\n",
    "        ret=re.inserted_id    ##re에 id 번호 부여하는 것?\n",
    "        docNum += 1\n",
    "    except:\n",
    "        ret=doc['ProfileID']\n",
    "          \n",
    "    return ret\n",
    "#%%\n",
    "def clean_item(item):\n",
    "    item = item.replace('\\n', ' ')\n",
    "    item = item.strip()\n",
    "    return item\n",
    "#%%\n",
    "# https://www.linkedin.com/search/results/people/?facetGeoRegion=%5B%22us%3A0%22%5D&keywords=Data%20Engineer&origin=FACETED_SEARCH\n",
    "#검색하고자 하는 정보를 title를 넣어 기본 검색 URL을 만든다.\n",
    "# 잡타이틀 정보에 불필요한 공백, 특수문자를 제거한다.\n",
    "# 내가 가져올 url을 넘겨준다. scrape_url\n",
    "def generate_scrape_url(scrape_url, config):\n",
    "\n",
    "    title = config['Job Title']  ## config는 cfg.json전체 정보.\n",
    "\n",
    "# collecting people in US only\n",
    "    scrape_url += \"/search/results/people/?facetGeoRegion=%5B%22us%3A0%22%5D&keywords=\"\n",
    "    scrape_url += title\n",
    "    scrape_url += \"&origin=SWITCH_SEARCH_VERTICAL\"\n",
    "\n",
    "\n",
    "    valid_title_name = title.strip().replace(' ', '_')\n",
    "    valid_title_name = re.sub(r'(?u)[^-\\w.]', '', valid_title_name) #means 첫번째를 2번째로 바꾼다. valid_title_name에서\n",
    "\n",
    "    return scrape_url\n",
    "#%%\n",
    "def scroll_down_page(browser, speed=8): \n",
    "    current_scroll_position, new_height = 0, 1\n",
    "    while current_scroll_position <= new_height:\n",
    "        current_scroll_position += speed\n",
    "        browser.execute_script(\"window.scrollTo(0, {});\".format(current_scroll_position))\n",
    "        new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "#%%\n",
    "def pscrape(config):\n",
    "    start_time = time.time()\n",
    "    # driver download: https://github.com/mozilla/geckodriver/releases\n",
    "    # windows \\, Linux and Max /\n",
    "    driver = os.getcwd() + \"/geckodriver.exe\"\n",
    "    base_url = \"https://www.linkedin.com\"\n",
    "    sign_in_url = \"https://www.linkedin.com/uas/login?fromSignIn=true\"\n",
    "    people_data = []\n",
    "    page = 1\n",
    "\n",
    "    USERNAME = config['User Name']\n",
    "    PASSWORD = config['Password']\n",
    "    col=config['Collection']\n",
    "    \n",
    "    # 주어진 json파일정보로 검색 url을 얻기\n",
    "    people_search_url = generate_scrape_url(base_url, config)\n",
    "\n",
    "    # 해당 URL 로 이동 (로그인 페이지 이동)\n",
    "    print('\\nSTATUS: Opening website')\n",
    "    browser = webdriver.Firefox(executable_path=driver)  ##내가 Firefox에서 Chrome으로 바꿈\n",
    "    browser.get(sign_in_url)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # 해당 페이지에서 유저명과 패스워드를 입력.\n",
    "    print('STATUS: Signing in')\n",
    "    browser.find_element_by_id('username').send_keys(USERNAME)\n",
    "    time.sleep(1)\n",
    "\n",
    "    browser.find_element_by_id('password').send_keys(PASSWORD)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # 로그인 버튼 클릭\n",
    "    browser.find_element_by_class_name('login__form_action_container ').click()\n",
    "    time.sleep(1)\n",
    " \n",
    "    # 로그인 이후에 json에 주어진 jobtitle로 검색을 수행한다. 수행 후, 스크롤해주기\n",
    "    print('STATUS: Searching for people\\n')\n",
    "    browser.get(people_search_url)\n",
    "    time.sleep(2)\n",
    "    scroll_down_page(browser, 4)\n",
    "\n",
    "    # 사람 정보 위치(아마도)\n",
    "    people = browser.find_elements_by_xpath('//div[@class=\"search-result__info pt3 pb4 ph0\"]')\n",
    "\n",
    "    \n",
    "    if len(people) == 0:  # 만약 원하는 위치에 정보가 없으면\n",
    "        print('STATUS: No people found. Press any key to exit scraper')\n",
    "        print(\"Check docnum.txt for # of documents submitted!\")\n",
    "        # docnum.txt를 만들어서(또는 열어서) 정보를 써준다.\n",
    "        today = datetime.now()\n",
    "        f = open(\"docnum1.txt\",\"w+\")  ##docnum.txt 없으면 생기고, 쓰기 기능으로(+ 조사 필요)들어놓아야 하는지? 코드와 같은 Anaconda Folder 안에-\n",
    "        f.write(\"Ran:\\n\")\n",
    "        f.write(str(today))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\nNumber of documents submitted:\\n\")\n",
    "        f.write(str(docNum))\n",
    "        f.close()\n",
    "        browser.quit()\n",
    "        exit = input('')  # 어떤 역할인지? 우선은 skip(우선은 사용하는 곳이 없어보임.)\n",
    "        sys.exit(0)  # \n",
    "\n",
    "    # 위치에 정보가 있으면\n",
    "    \n",
    "    #Scraping up to 6 pages per search.\n",
    "    while True and page != 11:\n",
    "        print('STATUS: Scraping Page ' + str(page))\n",
    "        \n",
    "        # 링크가져오기\n",
    "        links = []\n",
    "        for link in people:\n",
    "            try:\n",
    "                link = link.find_element_by_class_name(\"search-result__result-link\").get_attribute(\"href\")\n",
    "                links.append(link)\n",
    "            except:\n",
    "                ()\n",
    "                \n",
    "        # 링크를 얻고 바로 그 주소로 이동.(획득한 링크만큼)\n",
    "        for link in links:\n",
    "            obj = {}  # 딕셔너리 형태로 빈 딕셔너리 생성.\n",
    "            browser.get(link)\n",
    "            time.sleep(2)\n",
    "\n",
    "            # 20초동안 스크롤을 한다.\n",
    "            scroll_down_page(browser, 20)\n",
    "  \n",
    "            # 아마도, 링크안에 ProfileID 관련 정보가 있다. 그래서 그 정보를 가져오기\n",
    "            # obj : ProfileID 키 추가, \n",
    "            # case1. 값이 만약 people일 아닌 경우,\n",
    "            obj['ProfileID'] = link.split('/')[len(link.split('/'))-2]\n",
    "            if obj['ProfileID'] != \"people\":\n",
    "                print(\"STATUS: Scraping Profile_ID: {}\".format(obj['ProfileID']))\n",
    "\n",
    "                # 무언가 클릭을 두번 한다. \n",
    "                try:\n",
    "                    browser.find_element_by_xpath(\"//a[@class='lt-line-clamp__more']\").click()\n",
    "                except:\n",
    "                    ()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath(\"//button[@class='pv-profile-section__see-more-inline pv-profile-section__text-truncate-toggle link link-without-hover-state']\").click()\n",
    "                except:\n",
    "                    ()\n",
    "\n",
    "                # 어떤 요소의 Job Title를 text로 가져온다.\n",
    "                try:\n",
    "                    obj['Job Title'] = clean_item(browser.find_element_by_xpath(\"//h2[@class='mt1 t-18 t-black t-normal']\").text)\n",
    "                except:\n",
    "                    obj['Job Title'] = ''\n",
    "\n",
    "                # 어떤 요소의 Location을 가져온다. \n",
    "                try:\n",
    "                    obj['Location'] = clean_item(browser.find_element_by_xpath(\"//li[@class='t-16 t-black t-normal inline-block']\").text)\n",
    "                except:\n",
    "                    obj['Location'] = ''\n",
    "\n",
    "                # Profile Summary를 가져오기 \n",
    "                try:\n",
    "                    obj['Profile Summary'] = clean_item(browser.find_element_by_class_name(\"pv-about__summary-text\").text)\n",
    "                except:\n",
    "                    obj['Profile Summary'] = ''\n",
    "                \n",
    "                # 회사를 가져오기(근무했던 했던 회사가 아닐까?)\n",
    "                try:\n",
    "                    companies = browser.find_element_by_id(\"experience-section\").find_elements_by_class_name(\"pv-profile-section__card-item-v2\")\n",
    "                except:\n",
    "                    companies = []\n",
    "\n",
    "                experience = []\n",
    "                experience_obj = {}\n",
    "                \n",
    "                # 회사들(경력)에서 상세 정보가져오기\n",
    "                for company in companies:\n",
    "                    try:\n",
    "                        experience_obj['Job Title'] = clean_item(company.find_element_by_tag_name('h3').text)\n",
    "                    except:\n",
    "                        experience_obj['Job Title'] = ''\n",
    "\n",
    "                    try:\n",
    "                        experience_obj['Company'] = clean_item(company.find_element_by_class_name('pv-entity__secondary-title').text)\n",
    "                    except:\n",
    "                        experience_obj['Company'] = ''\n",
    "\n",
    "                    try:\n",
    "                        experience_obj['Period'] = clean_item(company.find_element_by_class_name('pv-entity__date-range').text).replace('Dates Employed ', '')\n",
    "                    except:\n",
    "                        experience_obj['Period'] = ''\n",
    "\n",
    "                    try:\n",
    "                        experience_obj['Years'] = clean_item(company.find_element_by_class_name(\"pv-entity__bullet-item-v2\").text)\n",
    "                    except:\n",
    "                        experience_obj['Years'] = ''\n",
    "\n",
    "                    try:\n",
    "                        experience_obj['Location'] = clean_item(company.find_element_by_class_name('pv-entity__location').text).replace('Location ', '')\n",
    "                    except:\n",
    "                        experience_obj['Location'] = ''\n",
    "\n",
    "                    try:\n",
    "                        experience_obj['Description'] = clean_item(company.find_element_by_class_name('pv-entity__description').text)\n",
    "                    except:\n",
    "                        experience_obj['Description'] = ''\n",
    "\n",
    "                    experience.append(experience_obj)\n",
    "                    experience_obj = {}\n",
    "\n",
    "                obj['Experience'] = experience\n",
    "\n",
    "                # 어떤 교육(대학 또는 기관)을 받았는가?\n",
    "                try:\n",
    "                    institutes = browser.find_element_by_id(\"education-section\").find_elements_by_class_name(\"pv-entity__summary-info\")\n",
    "                except:\n",
    "                    institutes = []\n",
    "\n",
    "                education = []\n",
    "                education_obj = {}\n",
    "\n",
    "                # 어떤 학교?(school) 전공일 것 같음. 아마. \n",
    "                for institute in institutes:\n",
    "                    try:\n",
    "                        education_obj['School'] = clean_item(institute.find_element_by_xpath(\"//h3[@class='pv-entity__school-name t-16 t-black t-bold']\").text)\n",
    "                    except:\n",
    "                        education_obj['School'] = ''\n",
    "\n",
    "                    try:\n",
    "                        degree_name = clean_item(institute.find_element_by_class_name('pv-entity__degree-name').text).replace('Degree Name ', '')\n",
    "                    except:\n",
    "                        degree_name = ''\n",
    "\n",
    "                    try:\n",
    "                        field_of_study = clean_item(institute.find_element_by_class_name('pv-entity__fos').text).replace('Field Of Study ', '')\n",
    "                    except:\n",
    "                        field_of_study = ''\n",
    "\n",
    "                    try:\n",
    "                        grade = clean_item(institute.find_element_by_class_name('pv-entity__grade').text).replace('Grade ', '')\n",
    "                    except:\n",
    "                        grade = ''\n",
    "\n",
    "                    education_obj['Degree'] = degree_name+' '+field_of_study+' '+grade\n",
    "\n",
    "                    try:\n",
    "                        education_obj['Date Attended'] = clean_item(institute.find_element_by_class_name('pv-entity__dates').text).replace('Dates attended or expected graduation ', '')\n",
    "                    except:\n",
    "                        education_obj['Date Attended'] = ''\n",
    "\n",
    "                    education.append(education_obj)\n",
    "                    education_obj = {}\n",
    "\n",
    "                obj['Education'] = education\n",
    "            \n",
    "                try:\n",
    "                    browser.find_element_by_xpath(\"//button[@class='pv-profile-section__card-action-bar pv-skills-section__additional-skills artdeco-container-card-action-bar artdeco-button artdeco-button--tertiary artdeco-button--3 artdeco-button--fluid']\").click()\n",
    "                except:\n",
    "                    print(\"No Skills\")\n",
    "\n",
    "                try:\n",
    "                    skill_sets = browser.find_element_by_class_name(\"pv-skill-categories-section__top-skills\").find_elements_by_class_name(\"pv-skill-category-entity__skill-wrapper\")\n",
    "                except:\n",
    "                    skill_sets = []\n",
    "\n",
    "                skills = []\n",
    "                skills_obj = {}\n",
    "\n",
    "                # 당신이 가진 기술은 (아마 확인 필요. Python, R, Spark, ...)\n",
    "                for skill_set in skill_sets:\n",
    "                    try:\n",
    "                        skills_obj['Skills'] = clean_item(skill_set.find_element_by_class_name('pv-skill-category-entity__name').text)\n",
    "                    except:\n",
    "                        skills_obj['Skills'] = ''\n",
    "\n",
    "                    skills.append(skills_obj)\n",
    "                    skills_obj = {}\n",
    "\n",
    "                obj['Skills & Endorsements'] = skills\n",
    "\n",
    "                try:\n",
    "                    all_skillsets = browser.find_elements_by_xpath(\"//div[@class='pv-skill-category-list pv-profile-section__section-info mb6 ember-view']\")\n",
    "                except:\n",
    "                    all_skillsets = []\n",
    "\n",
    "                # 스킬들에 정보를 하나씩 뽑아내기\n",
    "                for one in all_skillsets:\n",
    "                    general_skills = []\n",
    "                    try:\n",
    "                        g_skills = one.find_elements_by_class_name(\"pv-skill-category-entity\")\n",
    "                        for g_skill in g_skills:\n",
    "                            try:\n",
    "                                skills_obj['Skills'] = clean_item(\n",
    "                                    g_skill.find_element_by_class_name('pv-skill-category-entity__name').text)\n",
    "                            except:\n",
    "                                skills_obj['Skills'] = ''\n",
    "\n",
    "                            general_skills.append(skills_obj)\n",
    "                            skills_obj = {}\n",
    "                    except:\n",
    "                        general_skills = []\n",
    "\n",
    "                    try:\n",
    "                        category_name = clean_item(one.find_element_by_tag_name('h3').text)\n",
    "                    except:\n",
    "                        category_name = \"Skills\"\n",
    "\n",
    "                    obj[category_name] = general_skills\n",
    "\n",
    "                try:\n",
    "                    dateCaptured = date.today()\n",
    "                    obj['Date Captured'] = str(dateCaptured)\n",
    "                except:\n",
    "                    obj['Date Captured'] = ''\n",
    "\n",
    "                # 이제 모아진 정보 (obj에 모였음) 이를 mongodb에 넣는다.\n",
    "                people_data.append(obj)\n",
    "            \n",
    "                doc_id=mongodb_put_doc(obj)   # 넣기\n",
    "                print('post id: ', doc_id)\n",
    "            else:\n",
    "                print(\"STATUS: Skipping Profile_ID: {}\".format(obj['ProfileID']))\n",
    "\n",
    "        page += 1\n",
    "        \n",
    "        # 다음 페이지 넘기기\n",
    "        next_page = people_search_url + '&page=' + str(page)\n",
    "        browser.get(next_page)\n",
    "        time.sleep(2)\n",
    "        scroll_down_page(browser, 4)\n",
    "\n",
    "\n",
    "        people = browser.find_elements_by_xpath('//div[@class=\"search-result__info pt3 pb4 ph0\"]')\n",
    "        if len(people) == 0:\n",
    "            break\n",
    "\n",
    "        # 앞의 while문에 의해서 아마 6페이지를 가져오는 소스 코드임.\n",
    "    browser.quit()\n",
    "#%%\n",
    "import os\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Reads in the config file so input is automatic.\n",
    "    with open(\"cfg.json\") as json_cfg:\n",
    "        d = json.load(json_cfg)\n",
    "    \n",
    "    jobTitle = d['Job Title']\n",
    "\n",
    "    jobList = []\n",
    "    tempStr = \"\"\n",
    "\n",
    "    pscrape(d)\n",
    "    # while len(configArray) > 0:\n",
    "    #     try:\n",
    "    #         int(configArray[0])\n",
    "    #         break\n",
    "    #     except:\n",
    "    #         jobList.append(configArray[0])\n",
    "    #         del configArray[0]\n",
    "\n",
    "    # # Searches multiple jobs in a row.\n",
    "    # for x in range(len(jobList)):\n",
    "    #     print(\"Now scraping:\", jobList[0])\n",
    "    #     pscrape(jobList[0], configArray)\n",
    "    #     time.sleep(5)\n",
    "    #     del jobList[0]\n",
    "\n",
    "    print(\"Daily automation has been completed for: get_people.py\")\n",
    "    print(\"Check docnum.txt for # of documents submitted!\")\n",
    "    today = datetime.now()\n",
    "    f = open(\"docnum.txt\",\"w+\")\n",
    "    f.write(\"Ran:\\n\")\n",
    "    f.write(str(today))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\nNumber of documents submitted:\\n\")\n",
    "    f.write(str(docNum))\n",
    "    f.close()\n",
    "\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
