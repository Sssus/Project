{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STATUS: Opening website\n",
      "STATUS: Signing in\n",
      "STATUS: Searching for people\n",
      "\n",
      "STATUS: Scraping Page 90\n",
      "STATUS: Scraping Profile_ID: monica-reddy\n",
      "post id:  5dce1cb8c01402fda6e8ecbe\n",
      "STATUS: Scraping Profile_ID: atefeh-malekinezhad\n",
      "post id:  5dce1cc1c01402fda6e8ecc0\n",
      "STATUS: Scraping Profile_ID: mukai-nong-276391b0\n",
      "post id:  5dce1cccc01402fda6e8ecc2\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Scraping Profile_ID: kirangantal\n",
      "post id:  5dce1cdec01402fda6e8ecc4\n",
      "STATUS: Scraping Profile_ID: wenwan-vivian-yang-43380118\n",
      "post id:  5dce1ce8c01402fda6e8ecc6\n",
      "STATUS: Scraping Profile_ID: vinodhreddyv\n",
      "post id:  5dce1cf2c01402fda6e8ecc8\n",
      "STATUS: Scraping Profile_ID: floodmatthew\n",
      "post id:  5dce1cfbc01402fda6e8ecca\n",
      "STATUS: Scraping Profile_ID: pradeepkoganti\n",
      "post id:  5dce1d04c01402fda6e8eccc\n",
      "STATUS: Scraping Profile_ID: krutika-kothawale\n",
      "post id:  5dce1d0cc01402fda6e8ecce\n",
      "STATUS: Scraping Page 91\n",
      "STATUS: Scraping Profile_ID: paultluczek\n",
      "post id:  5dce1d25c01402fda6e8ecd0\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Scraping Profile_ID: shilpa-ranjan-2b853152\n",
      "post id:  5dce1d39c01402fda6e8ecd2\n",
      "STATUS: Scraping Profile_ID: naveenmylarappa\n",
      "post id:  5dce1d43c01402fda6e8ecd4\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Scraping Profile_ID: pavanjalla\n",
      "post id:  5dce1d55c01402fda6e8ecd6\n",
      "STATUS: Scraping Profile_ID: profphreak\n",
      "post id:  5dce1d62c01402fda6e8ecd8\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Scraping Profile_ID: stadinada\n",
      "post id:  5dce1d77c01402fda6e8ecda\n",
      "STATUS: Scraping Profile_ID: toneykim\n",
      "post id:  5dce1d7fc01402fda6e8ecdc\n",
      "STATUS: Scraping Page 92\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Scraping Profile_ID: ray-ruichen-wang-620a7748\n",
      "post id:  5dce1d99c01402fda6e8ecde\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Scraping Profile_ID: jefftarn\n",
      "post id:  5dce1daac01402fda6e8ece0\n",
      "STATUS: Scraping Profile_ID: robertdannels\n",
      "post id:  5dce1db8c01402fda6e8ece2\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Scraping Profile_ID: iamyashashah\n",
      "post id:  5dce1dd0c01402fda6e8ece4\n",
      "STATUS: Scraping Profile_ID: amit-parmar-93b03257\n",
      "post id:  5dce1dddc01402fda6e8ece6\n",
      "STATUS: Scraping Profile_ID: sudha-subramanian\n",
      "post id:  5dce1df1c01402fda6e8ece8\n",
      "STATUS: Scraping Profile_ID: vivilerongchen\n",
      "post id:  5dce1e03c01402fda6e8ecea\n",
      "STATUS: Scraping Page 93\n",
      "STATUS: Scraping Profile_ID: krystalwang0708\n",
      "post id:  5dce1e1ec01402fda6e8ecec\n",
      "STATUS: Scraping Profile_ID: hwanryuljo\n",
      "post id:  5dce1e2ec01402fda6e8ecee\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Scraping Profile_ID: gupta-himanshu-data-enthusiast\n",
      "post id:  5dce1e75c01402fda6e8ecf0\n",
      "STATUS: Scraping Profile_ID: pramitm\n",
      "post id:  5dce1e89c01402fda6e8ecf2\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Scraping Profile_ID: parmeet-singh\n",
      "post id:  5dce1eb9c01402fda6e8ecf4\n",
      "STATUS: Scraping Profile_ID: dennisdt\n",
      "post id:  5dce1ec5c01402fda6e8ecf6\n",
      "STATUS: Scraping Page 94\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Scraping Profile_ID: ravi-raisinghani\n",
      "No Skills\n",
      "post id:  5dce1ee0c01402fda6e8ecf8\n",
      "STATUS: Scraping Profile_ID: kanhu-badtia-a9898a20\n",
      "post id:  5dce1f0fc01402fda6e8ecfa\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Scraping Profile_ID: shivalisohoni\n",
      "post id:  5dce1f2ac01402fda6e8ecfc\n",
      "STATUS: Scraping Profile_ID: anurag-patil\n",
      "post id:  5dce1f3dc01402fda6e8ecfe\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Scraping Profile_ID: qiyue-ma-79604866\n",
      "post id:  5dce1f65c01402fda6e8ed00\n",
      "STATUS: Scraping Profile_ID: om-prakash-a66b4521\n",
      "post id:  5dce1f72c01402fda6e8ed02\n",
      "STATUS: Scraping Profile_ID: prajapatipranav\n",
      "post id:  5dce1f86c01402fda6e8ed04\n",
      "STATUS: Scraping Page 95\n",
      "STATUS: Scraping Profile_ID: jasonrobinsonphd\n",
      "post id:  5dce1fa9c01402fda6e8ed06\n",
      "STATUS: Scraping Profile_ID: monishakanoth\n",
      "post id:  5dce1fb6c01402fda6e8ed08\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Scraping Profile_ID: arjun-sehgal-5b0974a3\n",
      "post id:  5dce1fd6c01402fda6e8ed0a\n",
      "STATUS: Scraping Profile_ID: naveenrajkurapati\n",
      "post id:  5dce1ffdc01402fda6e8ed0c\n",
      "STATUS: Scraping Profile_ID: gajadata\n",
      "post id:  5dce2017c01402fda6e8ed0e\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Scraping Profile_ID: kamalapriya-anandaraman-akp1610\n",
      "post id:  5dce2040c01402fda6e8ed10\n",
      "STATUS: Scraping Page 96\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Scraping Profile_ID: djaiman-28\n",
      "post id:  5dce2075c01402fda6e8ed12\n",
      "STATUS: Scraping Profile_ID: neelima-gaddam-a6a69621\n",
      "post id:  5dce2086c01402fda6e8ed14\n",
      "STATUS: Scraping Profile_ID: nabil-abbas\n",
      "post id:  5dce209dc01402fda6e8ed16\n",
      "STATUS: Scraping Profile_ID: 123mmm\n",
      "post id:  5dce20acc01402fda6e8ed18\n",
      "STATUS: Scraping Profile_ID: dustinvannoy\n",
      "post id:  5dce20bfc01402fda6e8ed1a\n",
      "STATUS: Scraping Profile_ID: larskamp\n",
      "post id:  5dce20e1c01402fda6e8ed1c\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Scraping Profile_ID: sqlrahul\n",
      "post id:  5dce20f9c01402fda6e8ed1e\n",
      "STATUS: Scraping Profile_ID: mahsarostami\n",
      "post id:  5dce2106c01402fda6e8ed20\n",
      "STATUS: Scraping Page 97\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Scraping Profile_ID: chaitasee\n",
      "post id:  5dce212dc01402fda6e8ed22\n",
      "STATUS: Skipping Profile_ID: people\n",
      "STATUS: Scraping Profile_ID: navdeep-gill-b1729456\n",
      "post id:  5dce2145c01402fda6e8ed24\n",
      "STATUS: Scraping Profile_ID: dushyanthbala\n",
      "post id:  5dce2151c01402fda6e8ed26\n",
      "STATUS: Scraping Profile_ID: vivek-gautam-23371437\n",
      "post id:  5dce2166c01402fda6e8ed28\n",
      "STATUS: Scraping Profile_ID: julie-rice-56919156\n",
      "post id:  5dce2178c01402fda6e8ed2a\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: Failed to decode response from marionette\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6715cfd2201c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[0mtempStr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m     \u001b[0mpscrape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m     \u001b[1;31m# while len(configArray) > 0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m     \u001b[1;31m#     try:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-6715cfd2201c>\u001b[0m in \u001b[0;36mpscrape\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;31m# 20초동안 스크롤을 한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m             \u001b[0mscroll_down_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[1;31m# 아마도, 링크안에 ProfileID 관련 정보가 있다. 그래서 그 정보를 가져오기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-6715cfd2201c>\u001b[0m in \u001b[0;36mscroll_down_page\u001b[1;34m(browser, speed)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mcurrent_scroll_position\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mnew_height\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mcurrent_scroll_position\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mspeed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"window.scrollTo(0, {});\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_scroll_position\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mnew_height\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"return document.body.scrollHeight\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute_script\u001b[1;34m(self, script, *args)\u001b[0m\n\u001b[0;32m    634\u001b[0m         return self.execute(command, {\n\u001b[0;32m    635\u001b[0m             \u001b[1;34m'script'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m             'args': converted_args})['value']\n\u001b[0m\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexecute_async_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: Failed to decode response from marionette\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Nov  9 17:29:50 2019\n",
    "\n",
    "@author: Sungmin Hong\n",
    "\"\"\"\n",
    "#%%\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from pymongo import MongoClient\n",
    "#%%\n",
    "# Global Variable\n",
    "docNum = 0\n",
    "#%%\n",
    "# REPLACE With your LinkedIn Credentials\n",
    "USERNAME = \"\"\n",
    "PASSWORD = \"\"\n",
    "#%%\n",
    "def mongodb_init():\n",
    "    client=MongoClient('mongodb://localhost')\n",
    "    db=client.smartcareer\n",
    "    return db\n",
    "#%%\n",
    "def mongodb_get_collection(db,item):\n",
    "    col=db[item]\n",
    "    return col\n",
    "#%%\n",
    "def mongodb_put_doc(doc):\n",
    "    db=mongodb_init()\n",
    "    col=mongodb_get_collection(db,'dataengineer')\n",
    "\n",
    "    try:\n",
    "        global docNum\n",
    "        re=col.insert_one(doc)\n",
    "        ret=re.inserted_id    ##re에 id 번호 부여하는 것?\n",
    "        docNum += 1\n",
    "    except:\n",
    "        ret=doc['ProfileID']\n",
    "          \n",
    "    return ret\n",
    "#%%\n",
    "def clean_item(item):\n",
    "    item = item.replace('\\n', ' ')\n",
    "    item = item.strip()\n",
    "    return item\n",
    "#%%\n",
    "# https://www.linkedin.com/search/results/people/?facetGeoRegion=%5B%22us%3A0%22%5D&keywords=Data%20Engineer&origin=FACETED_SEARCH\n",
    "#검색하고자 하는 정보를 title를 넣어 기본 검색 URL을 만든다.\n",
    "# 잡타이틀 정보에 불필요한 공백, 특수문자를 제거한다.\n",
    "# 내가 가져올 url을 넘겨준다. scrape_url\n",
    "def generate_scrape_url(scrape_url, config):\n",
    "\n",
    "    title = config['Job Title']  ## config는 cfg.json전체 정보.\n",
    "\n",
    "# collecting people in US only\n",
    "    scrape_url += \"/search/results/people/?facetGeoRegion=%5B%22us%3A0%22%5D&keywords=\"\n",
    "    scrape_url += title\n",
    "    scrape_url += \"&origin=SWITCH_SEARCH_VERTICAL&page=90\"\n",
    "\n",
    "\n",
    "    valid_title_name = title.strip().replace(' ', '_')\n",
    "    valid_title_name = re.sub(r'(?u)[^-\\w.]', '', valid_title_name) #means 첫번째를 2번째로 바꾼다. valid_title_name에서\n",
    "\n",
    "    return scrape_url\n",
    "#%%\n",
    "def scroll_down_page(browser, speed=8): \n",
    "    current_scroll_position, new_height = 0, 1\n",
    "    while current_scroll_position <= new_height:\n",
    "        current_scroll_position += speed\n",
    "        browser.execute_script(\"window.scrollTo(0, {});\".format(current_scroll_position))\n",
    "        new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "#%%\n",
    "def pscrape(config):\n",
    "    start_time = time.time()\n",
    "    # driver download: https://github.com/mozilla/geckodriver/releases\n",
    "    # windows \\, Linux and Max /\n",
    "    driver = os.getcwd() + \"/geckodriver.exe\"\n",
    "    base_url = \"https://www.linkedin.com\"\n",
    "    sign_in_url = \"https://www.linkedin.com/uas/login?fromSignIn=true\"\n",
    "    people_data = []\n",
    "    page = 90\n",
    "\n",
    "    USERNAME = config['User Name']\n",
    "    PASSWORD = config['Password']\n",
    "    col=config['Collection']\n",
    "    \n",
    "    # 주어진 json파일정보로 검색 url을 얻기\n",
    "    people_search_url = generate_scrape_url(base_url, config)\n",
    "\n",
    "    # 해당 URL 로 이동 (로그인 페이지 이동)\n",
    "    print('\\nSTATUS: Opening website')\n",
    "    browser = webdriver.Firefox(executable_path=driver)  ##내가 Firefox에서 Chrome으로 바꿈\n",
    "    browser.get(sign_in_url)\n",
    "    time.sleep(random.randint(1,3))\n",
    "\n",
    "    # 해당 페이지에서 유저명과 패스워드를 입력.\n",
    "    print('STATUS: Signing in')\n",
    "    browser.find_element_by_id('username').send_keys(USERNAME)\n",
    "    time.sleep(random.randint(1,3))\n",
    "\n",
    "    browser.find_element_by_id('password').send_keys(PASSWORD)\n",
    "    time.sleep(random.randint(1,3))\n",
    "\n",
    "    # 로그인 버튼 클릭\n",
    "    browser.find_element_by_class_name('login__form_action_container ').click()\n",
    "    time.sleep(random.randint(1,3))\n",
    " \n",
    "    # 로그인 이후에 json에 주어진 jobtitle로 검색을 수행한다. 수행 후, 스크롤해주기\n",
    "    print('STATUS: Searching for people\\n')\n",
    "    browser.get(people_search_url)\n",
    "    time.sleep(random.randint(1,3))\n",
    "    scroll_down_page(browser, 4)\n",
    "\n",
    "    # 사람 정보 위치(아마도)\n",
    "    people = browser.find_elements_by_xpath('//div[@class=\"search-result__info pt3 pb4 ph0\"]')\n",
    "\n",
    "    \n",
    "    if len(people) == 0:  # 만약 원하는 위치에 정보가 없으면\n",
    "        print('STATUS: No people found. Press any key to exit scraper')\n",
    "        print(\"Check docnum.txt for # of documents submitted!\")\n",
    "        # docnum.txt를 만들어서(또는 열어서) 정보를 써준다.\n",
    "        today = datetime.now()\n",
    "        f = open(\"docnum.txt\",\"w+\")  ##docnum.txt 없으면 생기고, 쓰기 기능으로(+ 조사 필요)들어놓아야 하는지? 코드와 같은 Anaconda Folder 안에-\n",
    "        f.write(\"Ran:\\n\")\n",
    "        f.write(str(today))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\nNumber of documents submitted:\\n\")\n",
    "        f.write(str(docNum))\n",
    "        f.close()\n",
    "        browser.quit()\n",
    "        exit = input('')  # 어떤 역할인지? 우선은 skip(우선은 사용하는 곳이 없어보임.)\n",
    "        sys.exit(0)  # \n",
    "\n",
    "    # 위치에 정보가 있으면\n",
    "    \n",
    "    #Scraping up to 6 pages per search.\n",
    "    while True:\n",
    "        print('STATUS: Scraping Page ' + str(page))\n",
    "        \n",
    "        # 링크가져오기\n",
    "        links = []\n",
    "        for link in people:\n",
    "            try:\n",
    "                link = link.find_element_by_class_name(\"search-result__result-link\").get_attribute(\"href\")\n",
    "                links.append(link)\n",
    "            except:\n",
    "                ()\n",
    "                \n",
    "        # 링크를 얻고 바로 그 주소로 이동.(획득한 링크만큼)\n",
    "        for link in links:\n",
    "            obj = {}  # 딕셔너리 형태로 빈 딕셔너리 생성.\n",
    "            browser.get(link)\n",
    "            time.sleep(random.randint(1,3))\n",
    "\n",
    "            # 20초동안 스크롤을 한다.\n",
    "            scroll_down_page(browser, 20)\n",
    "  \n",
    "            # 아마도, 링크안에 ProfileID 관련 정보가 있다. 그래서 그 정보를 가져오기\n",
    "            # obj : ProfileID 키 추가, \n",
    "            # case1. 값이 만약 people일 아닌 경우,\n",
    "            obj['ProfileID'] = link.split('/')[len(link.split('/'))-2]\n",
    "            if obj['ProfileID'] != \"people\":\n",
    "                print(\"STATUS: Scraping Profile_ID: {}\".format(obj['ProfileID']))\n",
    "\n",
    "                # 무언가 클릭을 두번 한다. \n",
    "                try:\n",
    "                    browser.find_element_by_xpath(\"//a[@class='lt-line-clamp__more']\").click()\n",
    "                except:\n",
    "                    ()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath(\"//button[@class='pv-profile-section__see-more-inline pv-profile-section__text-truncate-toggle link link-without-hover-state']\").click()\n",
    "                except:\n",
    "                    ()\n",
    "\n",
    "                # 어떤 요소의 Job Title를 text로 가져온다.\n",
    "                try:\n",
    "                    obj['Job Title'] = clean_item(browser.find_element_by_xpath(\"//h2[@class='mt1 t-18 t-black t-normal']\").text)\n",
    "                except:\n",
    "                    obj['Job Title'] = ''\n",
    "\n",
    "                # 어떤 요소의 Location을 가져온다. \n",
    "                try:\n",
    "                    obj['Location'] = clean_item(browser.find_element_by_xpath(\"//li[@class='t-16 t-black t-normal inline-block']\").text)\n",
    "                except:\n",
    "                    obj['Location'] = ''\n",
    "\n",
    "                # Profile Summary를 가져오기 \n",
    "                try:\n",
    "                    obj['Profile Summary'] = clean_item(browser.find_element_by_class_name(\"pv-about__summary-text\").text)\n",
    "                except:\n",
    "                    obj['Profile Summary'] = ''\n",
    "                \n",
    "                # 회사를 가져오기(근무했던 했던 회사가 아닐까?)\n",
    "                try:\n",
    "                    companies = browser.find_element_by_id(\"experience-section\").find_elements_by_class_name(\"pv-profile-section__card-item-v2\")\n",
    "                except:\n",
    "                    companies = []\n",
    "\n",
    "                experience = []\n",
    "                experience_obj = {}\n",
    "                \n",
    "                # 회사들(경력)에서 상세 정보가져오기\n",
    "                for company in companies:\n",
    "                    try:\n",
    "                        experience_obj['Job Title'] = clean_item(company.find_element_by_tag_name('h3').text)\n",
    "                    except:\n",
    "                        experience_obj['Job Title'] = ''\n",
    "\n",
    "                    try:\n",
    "                        experience_obj['Company'] = clean_item(company.find_element_by_class_name('pv-entity__secondary-title').text)\n",
    "                    except:\n",
    "                        experience_obj['Company'] = ''\n",
    "\n",
    "                    try:\n",
    "                        experience_obj['Period'] = clean_item(company.find_element_by_class_name('pv-entity__date-range').text).replace('Dates Employed ', '')\n",
    "                    except:\n",
    "                        experience_obj['Period'] = ''\n",
    "\n",
    "                    try:\n",
    "                        experience_obj['Years'] = clean_item(company.find_element_by_class_name(\"pv-entity__bullet-item-v2\").text)\n",
    "                    except:\n",
    "                        experience_obj['Years'] = ''\n",
    "\n",
    "                    try:\n",
    "                        experience_obj['Location'] = clean_item(company.find_element_by_class_name('pv-entity__location').text).replace('Location ', '')\n",
    "                    except:\n",
    "                        experience_obj['Location'] = ''\n",
    "\n",
    "                    try:\n",
    "                        experience_obj['Description'] = clean_item(company.find_element_by_class_name('pv-entity__description').text)\n",
    "                    except:\n",
    "                        experience_obj['Description'] = ''\n",
    "\n",
    "                    experience.append(experience_obj)\n",
    "                    experience_obj = {}\n",
    "\n",
    "                obj['Experience'] = experience\n",
    "\n",
    "                # 어떤 교육(대학 또는 기관)을 받았는가?\n",
    "                try:\n",
    "                    institutes = browser.find_element_by_id(\"education-section\").find_elements_by_class_name(\"pv-entity__summary-info\")\n",
    "                except:\n",
    "                    institutes = []\n",
    "\n",
    "                education = []\n",
    "                education_obj = {}\n",
    "\n",
    "                # 어떤 학교?(school) 전공일 것 같음. 아마. \n",
    "                for institute in institutes:\n",
    "                    try:\n",
    "                        education_obj['School'] = clean_item(institute.find_element_by_xpath(\"//h3[@class='pv-entity__school-name t-16 t-black t-bold']\").text)\n",
    "                    except:\n",
    "                        education_obj['School'] = ''\n",
    "\n",
    "                    try:\n",
    "                        degree_name = clean_item(institute.find_element_by_class_name('pv-entity__degree-name').text).replace('Degree Name ', '')\n",
    "                    except:\n",
    "                        degree_name = ''\n",
    "\n",
    "                    try:\n",
    "                        field_of_study = clean_item(institute.find_element_by_class_name('pv-entity__fos').text).replace('Field Of Study ', '')\n",
    "                    except:\n",
    "                        field_of_study = ''\n",
    "\n",
    "                    try:\n",
    "                        grade = clean_item(institute.find_element_by_class_name('pv-entity__grade').text).replace('Grade ', '')\n",
    "                    except:\n",
    "                        grade = ''\n",
    "\n",
    "                    education_obj['Degree'] = degree_name+' '+field_of_study+' '+grade\n",
    "\n",
    "                    try:\n",
    "                        education_obj['Date Attended'] = clean_item(institute.find_element_by_class_name('pv-entity__dates').text).replace('Dates attended or expected graduation ', '')\n",
    "                    except:\n",
    "                        education_obj['Date Attended'] = ''\n",
    "\n",
    "                    education.append(education_obj)\n",
    "                    education_obj = {}\n",
    "\n",
    "                obj['Education'] = education\n",
    "            \n",
    "                try:\n",
    "                    browser.find_element_by_xpath(\"//button[@class='pv-profile-section__card-action-bar pv-skills-section__additional-skills artdeco-container-card-action-bar artdeco-button artdeco-button--tertiary artdeco-button--3 artdeco-button--fluid']\").click()\n",
    "                except:\n",
    "                    print(\"No Skills\")\n",
    "\n",
    "                try:\n",
    "                    skill_sets = browser.find_element_by_class_name(\"pv-skill-categories-section__top-skills\").find_elements_by_class_name(\"pv-skill-category-entity__skill-wrapper\")\n",
    "                except:\n",
    "                    skill_sets = []\n",
    "\n",
    "                skills = []\n",
    "                skills_obj = {}\n",
    "\n",
    "                # 당신이 가진 기술은 (아마 확인 필요. Python, R, Spark, ...)\n",
    "                for skill_set in skill_sets:\n",
    "                    try:\n",
    "                        skills_obj['Skills'] = clean_item(skill_set.find_element_by_class_name('pv-skill-category-entity__name').text)\n",
    "                    except:\n",
    "                        skills_obj['Skills'] = ''\n",
    "\n",
    "                    skills.append(skills_obj)\n",
    "                    skills_obj = {}\n",
    "\n",
    "                obj['Skills & Endorsements'] = skills\n",
    "\n",
    "                try:\n",
    "                    all_skillsets = browser.find_elements_by_xpath(\"//div[@class='pv-skill-category-list pv-profile-section__section-info mb6 ember-view']\")\n",
    "                except:\n",
    "                    all_skillsets = []\n",
    "\n",
    "                # 스킬들에 정보를 하나씩 뽑아내기\n",
    "                for one in all_skillsets:\n",
    "                    general_skills = []\n",
    "                    try:\n",
    "                        g_skills = one.find_elements_by_class_name(\"pv-skill-category-entity\")\n",
    "                        for g_skill in g_skills:\n",
    "                            try:\n",
    "                                skills_obj['Skills'] = clean_item(\n",
    "                                    g_skill.find_element_by_class_name('pv-skill-category-entity__name').text)\n",
    "                            except:\n",
    "                                skills_obj['Skills'] = ''\n",
    "\n",
    "                            general_skills.append(skills_obj)\n",
    "                            skills_obj = {}\n",
    "                    except:\n",
    "                        general_skills = []\n",
    "\n",
    "                    try:\n",
    "                        category_name = clean_item(one.find_element_by_tag_name('h3').text)\n",
    "                    except:\n",
    "                        category_name = \"Skills\"\n",
    "\n",
    "                    obj[category_name] = general_skills\n",
    "\n",
    "                try:\n",
    "                    dateCaptured = date.today()\n",
    "                    obj['Date Captured'] = str(dateCaptured)\n",
    "                except:\n",
    "                    obj['Date Captured'] = ''\n",
    "\n",
    "                # 이제 모아진 정보 (obj에 모였음) 이를 mongodb에 넣는다.\n",
    "                people_data.append(obj)\n",
    "            \n",
    "                doc_id=mongodb_put_doc(obj)   # 넣기\n",
    "                print('post id: ', doc_id)\n",
    "            else:\n",
    "                print(\"STATUS: Skipping Profile_ID: {}\".format(obj['ProfileID']))\n",
    "\n",
    "        page += 1\n",
    "        \n",
    "        # 다음 페이지 넘기기\n",
    "        next_page = people_search_url + '&page=' + str(page)\n",
    "        browser.get(next_page)\n",
    "        time.sleep(random.randint(1,3))\n",
    "        scroll_down_page(browser, 4)\n",
    "\n",
    "\n",
    "        people = browser.find_elements_by_xpath('//div[@class=\"search-result__info pt3 pb4 ph0\"]')\n",
    "        if len(people) == 0:\n",
    "            break\n",
    "\n",
    "        # 앞의 while문에 의해서 아마 6페이지를 가져오는 소스 코드임.\n",
    "    browser.quit()\n",
    "#%%\n",
    "import os\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Reads in the config file so input is automatic.\n",
    "    with open(\"cfg.json\") as json_cfg:\n",
    "        d = json.load(json_cfg)\n",
    "    \n",
    "    jobTitle = d['Job Title']\n",
    "\n",
    "    jobList = []\n",
    "    tempStr = \"\"\n",
    "\n",
    "    pscrape(d)\n",
    "    # while len(configArray) > 0:\n",
    "    #     try:\n",
    "    #         int(configArray[0])\n",
    "    #         break\n",
    "    #     except:\n",
    "    #         jobList.append(configArray[0])\n",
    "    #         del configArray[0]\n",
    "\n",
    "    # # Searches multiple jobs in a row.\n",
    "    # for x in range(len(jobList)):\n",
    "    #     print(\"Now scraping:\", jobList[0])\n",
    "    #     pscrape(jobList[0], configArray)\n",
    "    #     time.sleep(5)\n",
    "    #     del jobList[0]\n",
    "\n",
    "    print(\"Daily automation has been completed for: get_people.py\")\n",
    "    print(\"Check docnum.txt for # of documents submitted!\")\n",
    "    today = datetime.now()\n",
    "    f = open(\"docnum.txt\",\"w+\")\n",
    "    f.write(\"Ran:\\n\")\n",
    "    f.write(str(today))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\nNumber of documents submitted:\\n\")\n",
    "    f.write(str(docNum))\n",
    "    f.close()\n",
    "\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
